{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloaded directly from the website is in a format that can only be “read” using datasets.Food101 (split, transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_pie',\n",
       " 'baby_back_ribs',\n",
       " 'baklava',\n",
       " 'beef_carpaccio',\n",
       " 'beef_tartare',\n",
       " 'beet_salad',\n",
       " 'beignets',\n",
       " 'bibimbap',\n",
       " 'bread_pudding',\n",
       " 'breakfast_burrito',\n",
       " 'bruschetta',\n",
       " 'caesar_salad',\n",
       " 'cannoli',\n",
       " 'caprese_salad',\n",
       " 'carrot_cake',\n",
       " 'ceviche',\n",
       " 'cheese_plate',\n",
       " 'cheesecake',\n",
       " 'chicken_curry',\n",
       " 'chicken_quesadilla',\n",
       " 'chicken_wings',\n",
       " 'chocolate_cake',\n",
       " 'chocolate_mousse',\n",
       " 'churros',\n",
       " 'clam_chowder',\n",
       " 'club_sandwich',\n",
       " 'crab_cakes',\n",
       " 'creme_brulee',\n",
       " 'croque_madame',\n",
       " 'cup_cakes',\n",
       " 'deviled_eggs',\n",
       " 'donuts',\n",
       " 'dumplings',\n",
       " 'edamame',\n",
       " 'eggs_benedict',\n",
       " 'escargots',\n",
       " 'falafel',\n",
       " 'filet_mignon',\n",
       " 'fish_and_chips',\n",
       " 'foie_gras',\n",
       " 'french_fries',\n",
       " 'french_onion_soup',\n",
       " 'french_toast',\n",
       " 'fried_calamari',\n",
       " 'fried_rice',\n",
       " 'frozen_yogurt',\n",
       " 'garlic_bread',\n",
       " 'gnocchi',\n",
       " 'greek_salad',\n",
       " 'grilled_cheese_sandwich',\n",
       " 'grilled_salmon',\n",
       " 'guacamole',\n",
       " 'gyoza',\n",
       " 'hamburger',\n",
       " 'hot_and_sour_soup',\n",
       " 'hot_dog',\n",
       " 'huevos_rancheros',\n",
       " 'hummus',\n",
       " 'ice_cream',\n",
       " 'lasagna',\n",
       " 'lobster_bisque',\n",
       " 'lobster_roll_sandwich',\n",
       " 'macaroni_and_cheese',\n",
       " 'macarons',\n",
       " 'miso_soup',\n",
       " 'mussels',\n",
       " 'nachos',\n",
       " 'omelette',\n",
       " 'onion_rings',\n",
       " 'oysters',\n",
       " 'pad_thai',\n",
       " 'paella',\n",
       " 'pancakes',\n",
       " 'panna_cotta',\n",
       " 'peking_duck',\n",
       " 'pho',\n",
       " 'pizza',\n",
       " 'pork_chop',\n",
       " 'poutine',\n",
       " 'prime_rib',\n",
       " 'pulled_pork_sandwich',\n",
       " 'ramen',\n",
       " 'ravioli',\n",
       " 'red_velvet_cake',\n",
       " 'risotto',\n",
       " 'samosa',\n",
       " 'sashimi',\n",
       " 'scallops',\n",
       " 'seaweed_salad',\n",
       " 'shrimp_and_grits',\n",
       " 'spaghetti_bolognese',\n",
       " 'spaghetti_carbonara',\n",
       " 'spring_rolls',\n",
       " 'steak',\n",
       " 'strawberry_shortcake',\n",
       " 'sushi',\n",
       " 'tacos',\n",
       " 'takoyaki',\n",
       " 'tiramisu',\n",
       " 'tuna_tartare',\n",
       " 'waffles']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load entire data\n",
    "_ = datasets.Food101(root=Path.cwd().parent / \"Data\", download=True)\n",
    "\n",
    "# images path\n",
    "images_path = Path.cwd().parent / \"Data/food-101/images\"\n",
    "\n",
    "# only to display all classes\n",
    "all_data = datasets.ImageFolder(\n",
    "    root=images_path,\n",
    "    transform=transforms.ToTensor()  # This part will be conducted in dataloader.py for data prepared in cells below \n",
    ")\n",
    "\n",
    "all_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_TO_GET = 0.1\n",
    "N_FOLDS = 5\n",
    "TARGET_CLASSES = ['apple_pie',\n",
    " 'baby_back_ribs',\n",
    " 'baklava',\n",
    " 'beef_carpaccio',\n",
    " 'beef_tartare',\n",
    " 'beet_salad',\n",
    " 'beignets',\n",
    " 'bibimbap',\n",
    " 'bread_pudding',\n",
    " 'breakfast_burrito',\n",
    " 'bruschetta',\n",
    " 'caesar_salad',\n",
    " 'cannoli',\n",
    " 'caprese_salad',\n",
    " 'carrot_cake',\n",
    " 'ceviche',\n",
    " 'cheese_plate',\n",
    " 'cheesecake',\n",
    " 'chicken_curry',\n",
    " 'chicken_quesadilla',\n",
    " 'chicken_wings',\n",
    " 'chocolate_cake',\n",
    " 'chocolate_mousse',\n",
    " 'churros',\n",
    " 'clam_chowder',\n",
    " 'club_sandwich',\n",
    " 'crab_cakes',\n",
    " 'creme_brulee',\n",
    " 'croque_madame',\n",
    " 'cup_cakes',\n",
    " 'deviled_eggs',\n",
    " 'donuts',\n",
    " 'dumplings',\n",
    " 'edamame',\n",
    " 'eggs_benedict',\n",
    " 'escargots',\n",
    " 'falafel',\n",
    " 'filet_mignon',\n",
    " 'fish_and_chips',\n",
    " 'foie_gras',\n",
    " 'french_fries',\n",
    " 'french_onion_soup',\n",
    " 'french_toast',\n",
    " 'fried_calamari',\n",
    " 'fried_rice',\n",
    " 'frozen_yogurt',\n",
    " 'garlic_bread',\n",
    " 'gnocchi',\n",
    " 'greek_salad',\n",
    " 'grilled_cheese_sandwich',\n",
    " 'grilled_salmon',\n",
    " 'guacamole',\n",
    " 'gyoza',\n",
    " 'hamburger',\n",
    " 'hot_and_sour_soup',\n",
    " 'hot_dog',\n",
    " 'huevos_rancheros',\n",
    " 'hummus',\n",
    " 'ice_cream',\n",
    " 'lasagna',\n",
    " 'lobster_bisque',\n",
    " 'lobster_roll_sandwich',\n",
    " 'macaroni_and_cheese',\n",
    " 'macarons',\n",
    " 'miso_soup',\n",
    " 'mussels',\n",
    " 'nachos',\n",
    " 'omelette',\n",
    " 'onion_rings',\n",
    " 'oysters',\n",
    " 'pad_thai',\n",
    " 'paella',\n",
    " 'pancakes',\n",
    " 'panna_cotta',\n",
    " 'peking_duck',\n",
    " 'pho',\n",
    " 'pizza',\n",
    " 'pork_chop',\n",
    " 'poutine',\n",
    " 'prime_rib',\n",
    " 'pulled_pork_sandwich',\n",
    " 'ramen',\n",
    " 'ravioli',\n",
    " 'red_velvet_cake',\n",
    " 'risotto',\n",
    " 'samosa',\n",
    " 'sashimi',\n",
    " 'scallops',\n",
    " 'seaweed_salad',\n",
    " 'shrimp_and_grits',\n",
    " 'spaghetti_bolognese',\n",
    " 'spaghetti_carbonara',\n",
    " 'spring_rolls',\n",
    " 'steak',\n",
    " 'strawberry_shortcake',\n",
    " 'sushi',\n",
    " 'tacos',\n",
    " 'takoyaki',\n",
    " 'tiramisu',\n",
    " 'tuna_tartare',\n",
    " 'waffles']  # only chosen classes\n",
    "\n",
    "SPLITS = {\"train\": 0.7, \"val\": 0.15, \"test\": 0.15} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parent / \"Data\"\n",
    "split_str = \"_\".join(f\"{k[:2]}{int(v * 100)}\" for k, v in SPLITS.items())\n",
    "target_dir_name = data_path / f\"food-101_{str(int(AMOUNT_TO_GET*100))}%_{split_str}\"\n",
    "target_dir = Path(target_dir_name)\n",
    "\n",
    "target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save classes to txt inside folder\n",
    "with open(target_dir_name / \"classes.txt\", \"w\") as f:\n",
    "    for name in TARGET_CLASSES:\n",
    "        f.write(name + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION - select one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Extracting a subset of data from Food-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Data saved in splits under: /home/kamil-solski/Documents/Python/Projekty_py/Food101/Data/food-101_20%_tr70_va15_te15\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Split and copy\n",
    "for class_name in TARGET_CLASSES:\n",
    "    source_class_dir = images_path / class_name\n",
    "    all_images = list(source_class_dir.glob(\"*.jpg\"))\n",
    "    num_to_sample = int(len(all_images) * AMOUNT_TO_GET)\n",
    "\n",
    "    sampled_images = random.sample(all_images, num_to_sample)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(SPLITS[\"train\"] * num_to_sample)\n",
    "    val_end = train_end + int(SPLITS[\"val\"] * num_to_sample)\n",
    "\n",
    "    train_imgs = sampled_images[:train_end]\n",
    "    val_imgs = sampled_images[train_end:val_end]\n",
    "    test_imgs = sampled_images[val_end:]\n",
    "\n",
    "    split_map = {\n",
    "        \"train\": train_imgs,\n",
    "        \"val\": val_imgs,\n",
    "        \"test\": test_imgs\n",
    "    }\n",
    "\n",
    "    # Copy to corresponding split directories\n",
    "    for split_name, split_images in split_map.items():\n",
    "        split_class_dir = target_dir / split_name / class_name\n",
    "        split_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for img_path in split_images:\n",
    "            shutil.copy(img_path, split_class_dir / img_path.name)\n",
    "\n",
    "print(f\"Done! Data saved in splits under: {target_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Extracting a subset of data from Food101 (with K-Fold Cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! 5-fold cross-validation saved to: /home/kamil-solski/Documents/Python/Projekty_py/Food101_MLOps-Lvl_1/Data/food-101_10%_tr70_va15_te15\n"
     ]
    }
   ],
   "source": [
    "# Ensure reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# K-fold processing\n",
    "for class_name in TARGET_CLASSES:\n",
    "    source_class_dir = images_path / class_name\n",
    "    all_images = list(source_class_dir.glob(\"*.jpg\"))\n",
    "    num_to_sample = int(len(all_images) * AMOUNT_TO_GET)\n",
    "    sampled_images = random.sample(all_images, num_to_sample)\n",
    "\n",
    "    # Global test split\n",
    "    test_count = int(SPLITS[\"test\"] * len(sampled_images))\n",
    "    test_images = sampled_images[:test_count]\n",
    "    remaining_images = sampled_images[test_count:]\n",
    "\n",
    "    # Save global test set - without that we will \n",
    "    test_class_dir = target_dir / \"test\" / class_name\n",
    "    test_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for img in test_images:\n",
    "        shutil.copy(img, test_class_dir / img.name)\n",
    "\n",
    "    # K-Fold cross-validation on remaining images (train + val only)\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(remaining_images)):\n",
    "        fold_dir = target_dir / f\"fold{fold_idx}\"\n",
    "        fold_images = {\n",
    "            \"train\": [remaining_images[i] for i in train_idx],\n",
    "            \"val\": [remaining_images[i] for i in val_idx]\n",
    "        }\n",
    "\n",
    "        for split_name, split_images in fold_images.items():\n",
    "            split_class_dir = fold_dir / split_name / class_name\n",
    "            split_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for img in split_images:\n",
    "                shutil.copy(img, split_class_dir / img.name)\n",
    "                \n",
    "print(f\"Done! {N_FOLDS}-fold cross-validation saved to: {target_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Food101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
