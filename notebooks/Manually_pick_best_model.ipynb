{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4518d547",
   "metadata": {},
   "source": [
    "This part is done after training if Data Scientist would like to manually review which models are the best.\n",
    "\n",
    "### Parse mlflow to compute scores and get the best models\n",
    "- extract metadata fold-level, architecture and hyperparameter combination and filter out too big differences in losses (don't take overfitting ones)\n",
    "- compute averages across folds\n",
    "- compute scores\n",
    "- select the best score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07270a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e6f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_path_dir = Path.cwd().parent / \"experiments/mlruns\"\n",
    "mlflow.set_tracking_uri(mlflow_path_dir.as_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_runs(experiment_name: str) -> pd.DataFrame:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        raise ValueError(f\"Experiment '{experiment_name}' not found.\")\n",
    "    \n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        filter_string=\"attributes.status = 'FINISHED'\",\n",
    "        output_format=\"pandas\"\n",
    "    )\n",
    "    return runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = get_all_runs(\"food-101_30%_tr70_va15_te15_2025-08-05_17-22-11\")  # change to experiment you like to analyze\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_loss_discrepancy(runs: pd.DataFrame, threshold: float = 0.25) -> pd.DataFrame:\n",
    "    runs[\"loss_diff\"] = (runs[\"metrics.train_loss\"] - runs[\"metrics.val_loss\"]).abs()\n",
    "    return runs[runs[\"loss_diff\"] <= threshold].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42579509",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_runs = filter_by_loss_discrepancy(runs, threshold=0.25)\n",
    "filterd_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_arch_and_config(runs: pd.DataFrame, val_metric: str = \"val_acc\") -> pd.DataFrame:\n",
    "    grouped = runs.groupby([\"tags.architecture\", \"tags.config\"])\n",
    "\n",
    "    summary = []\n",
    "    for (arch, config), group in grouped:\n",
    "        avg_val_loss = group[\"metrics.val_loss\"].mean()\n",
    "        avg_val_metric = group[f\"metrics.{val_metric}\"].mean()\n",
    "\n",
    "        summary.append({\n",
    "            \"architecture\": arch,\n",
    "            \"config\": config,\n",
    "            \"folds\": len(group),\n",
    "            \"avg_val_loss\": avg_val_loss,\n",
    "            f\"avg_{val_metric}\": avg_val_metric\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9cd26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_runs = group_by_arch_and_config(filterd_runs, val_metric=\"val_acc\")\n",
    "grouped_runs  # there might not be all hyperparameter combinations if those with to big loss discrepancy were filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5930902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_models(df: pd.DataFrame, val_metric: str = \"val_acc\",\n",
    "                 acc_weight: float = 0.7, loss_weight: float = 0.3) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize\n",
    "    df[\"loss_score\"] = 1 - (df[\"avg_val_loss\"] - df[\"avg_val_loss\"].min()) / (df[\"avg_val_loss\"].max() - df[\"avg_val_loss\"].min())\n",
    "    df[\"metric_score\"] = (df[f\"avg_{val_metric}\"] - df[f\"avg_{val_metric}\"].min()) / (df[f\"avg_{val_metric}\"].max() - df[f\"avg_{val_metric}\"].min())\n",
    "\n",
    "    # Weighted score\n",
    "    df[\"score\"] = acc_weight * df[\"metric_score\"] + loss_weight * df[\"loss_score\"]\n",
    "    return df.sort_values(\"score\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ccb7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to check which one is the best by scoring algorithm (the same is in automated pipeline) run this cell:\n",
    "scored = score_models(grouped_runs, val_metric=\"val_acc\", acc_weight=0.7, loss_weight=0.3)  # you can modify weights based on how important you think those metrics sould be\n",
    "scored  # look at scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_configs(df_scored: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df_scored.loc[df_scored.groupby(\"architecture\")[\"score\"].idxmax()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = select_best_configs(scored)\n",
    "best_config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Food101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
